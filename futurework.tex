\section{Conclusion and Future Work}

The main goal of NumNN.jl is a software tool for simulating -in a simple but general way- neural networks models, with also the ability of adopting different number systems to be tested and assessed a priori of the hardware design process. Moreover, NumNN.jl in the future will be able to use different back-end devices and processing units to speed up the train and inference of neural networks. 

So far, the developed software package depends on predefined derivatives and gradients of the activation functions (each activation function has a derivative provided under the same name with the letter \mintinline{julia}{"d"} prefixed to its name like \mintinline{julia}{relu} and \mintinline{julia}{drelu}). The upcoming versions will be able to do automated derivation and gradient techniques.

More activation functions will be added and many new loss functions to serve the easiness and generality of this package. In the current status, NumNN.jl only accepts one input layer and one output layer, and multiple input and output layers will be also supported in the upcoming versions.