@misc{Julia,
title = {{The Julia Language}},
url = {https://julialang.org/},
urldate = {2020-04-19}
}
@misc{WikiMultipleDispatch,
title = {{Multiple dispatch - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Multiple_dispatch},
urldate = {2020-04-19}
}
@misc{Keras,
title = {{Home - Keras Documentation}},
url = {https://keras.io/},
urldate = {2020-04-21}
}
@misc{JuliaMehtods,
title = {{Julia Methods}},
url = {https://docs.julialang.org/en/v1/manual/methods/#Defining-Methods-1}
}
@inproceedings{Garner1959,
address = {New York, New York, USA},
author = {Garner, Harvey L.},
booktitle = {Papers presented at the the March 3-5, 1959, western joint computer conference on XX - IRE-AIEE-ACM '59 (Western)},
doi = {10.1145/1457838.1457864},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garner - 1959 - The residue number system.pdf:pdf},
pages = {146--153},
publisher = {ACM Press},
title = {{The residue number system}},
year = {1959}
}
@article{Kingsbury1971,
abstract = {A method of computation is described in which all signals are encoded logarithmically, giving a great improvement in dynamic range compared with fixed-point linearly encoded arithmetic. Two ways of adding or subtracting logarithmically encoded numbers are suggested, together with a logarithmic digital-analogue convenor. {\textcopyright} 1971, The Institution of Electrical Engineers. All rights reserved.},
author = {Kingsbury, N. G. and Rayner, P. J.W.},
doi = {10.1049/el:19710039},
issn = {0013-5194},
journal = {Electronics Letters},
keywords = {Filtering and prediction theory},
number = {2},
pages = {56--58},
title = {{Digital filtering using logarithmic arithmetic}},
volume = {7},
year = {1971}
}
@article{Alexopoulos1975,
abstract = {A signed logarithmic number system, which is capable of representing negative as well as positive numbers is described. A number is represented in the sign/logarithm number system by a sign bit and the logarithm of the absolute value of the number (scaled to avoid negative logarithms). Algorithms have been developed to perform all four of the basic arithmetic operations (i.e., addition, subtraction, multiplication, and division). It appears that such an arithmetic unit can be somewhat faster than a conventional arithmetic unit of comparable complexity. This system is intended for use in implementing special purpose computers, where constant relative accuracy is not objectionable. Copyright {\textcopyright} 1975 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Alexopoulos, Aristides G.},
doi = {10.1109/T-C.1975.224172},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Computer arithmetic units,logarithmic addition,logarithmic arithmetic,logarithmic subtraction,number systems},
number = {12},
pages = {1238--1242},
title = {{The Sign/Logarithm Number System}},
volume = {C-24},
year = {1975}
}
@article{Lee1977,
abstract = {This correspondence introduces a new numbersystem, called the Focus number system system which “focuses” on available resolution near zero much like an operational amplifier “focuses” on tiny deviations near virtual ground. This number system is supported by algorithms providing addition, multiplication, and higher order functions in a bare microcomputer at speeds rivaling those of existing integer arithmetic [1]–[3] performed with a costly arithmetic unit. {\textcopyright} 1977, IEEE. All rights reserved.},
author = {Lee, Samuel C. and Edgar, Albert D.},
doi = {10.1109/TC.1977.1674770},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Control systems,Logarithmic number system,Variable resolution encoding},
number = {11},
pages = {1167--1170},
title = {{The Focus Number System}},
volume = {C-26},
year = {1977}
}
@article{Jullien1987,
abstract = {In current high-speed digital signal-processing (DSP) architectures, the Residue Number System (RNS) has an important role to play. RNS implementations have a highly modular structure, and are not dependent upon large binary arithmetic elements. RNS implementations become more attractive when combined with the advantages offered by VLSI fabrication technology. In this paper, a novel design methodology has been developed for RNS structures, based on using look-up tables, which takes into consideration the unique features and requirements of RNS. The paper discusses the following three phases: 1) developing a look-up table layout model, which is used to derive relationships between the size of each modulus and both chip area and time; this model supports all types of moduli; 2) selecting the most efficient layout according to the design requirements; the procedure allows the designer to control the area, time, or the configuration of the memory module required for implementing a modulo look-up table; 3) proposing a set of multi-look-up table modules; to be used as building block units for implementing digital signal-processing architectures. The paper uses two examples to illustrate the use of the modules in phase 3). {\textcopyright} 1987 IEEE},
author = {Jullien, Graham A. and Miller, William C.},
doi = {10.1109/TCS.1987.1086188},
issn = {0098-4094},
journal = {IEEE Transactions on Circuits and Systems},
number = {6},
pages = {604--616},
title = {{A Look-Up Table VLSI Design Methodology for RNS Structures Used in DSP Applications}},
volume = {34},
year = {1987}
}
@inproceedings{DiClaudio1990,
abstract = {Fast RNS (residue number system) algorithms which use only binary arithmetic are developed. Scaled residues, called pseudoresidues, are introduced by exploiting the cycle properties of each RNS channel and solving a Diophantine equation. Using the pseudoresidues instead of the original residue set to perform the desired computations, an RNS processor can be built with standard binary devices of small wordlength. The effectiveness of the procedure is shown by developing the pseudoresidue implementations of a modular multiplier for odd moduli RNS and of a FIR (finite impulse response) filter. The resulting structures exhibit complete reprogrammability for both moduli and coefficients, a very low number of fast machine cycles, and a square time-area product reduction.},
author = {{Di Claudio}, E. D. and Orlandi, G. and Piazza, F.},
booktitle = {International Conference on Acoustics, Speech, and Signal Processing},
doi = {10.1109/icassp.1990.115701},
issn = {1520-6149},
pages = {1531--1534},
title = {{Fast RNS DSP algorithms implemented with binary arithmetic}},
volume = {3},
year = {1990}
}
@article{Lewis1995,
abstract = {An arithmetic core for DSP applications, comprising two multiplier/dividers and an adder/subtractor, using logarithmic number system (LNS) arithmetic, is described. For most operands, precision better than the worst-case precision of IEEE-754 is obtained. Three operations per cycle are performed using 69k transistors integrated in a 16 mm2 core in 1.2 µm CMOS, offering better performance than the best previous result in only 43% of the area. The use of a new interleaved memory ROM structure and a second-order function interpolator are the key techniques that result in reduced area. This modest area allows several core units to be integrated on a chip for high performance DSP applications. {\textcopyright} 1995 IEEE},
author = {Lewis, David M.},
doi = {10.1109/4.482205},
issn = {1558-173X},
journal = {IEEE Journal of Solid-State Circuits},
number = {12},
pages = {1547--1553},
title = {{114 MFLOPS Logarithmic Number System Arithmetic Unit for DSP Applications}},
volume = {30},
year = {1995}
}
@article{Claudio1995,
abstract = {It is known that RNS VLSI processors can parallelize fixed-point addition and multiplication operations by the use of the Chinese Remainder Theorem (CRT). The required modular operations, however, must use specialized hardware whose design and implementation can create several problems. In this paper a modified residue arithmetic, called pseudo-RNS is introduced in order to alleviate some of the RNS problems when Digital Signal Processing (DSP) structures are implemented. Pseudo-RNS requires only the use of modified binary processors and exhibits a speed performance comparable with other RNS traditional approaches. Some applications of the pseudo-RNS to common DSP architectures, such as multipliers and filters, are also presented in this paper. They are compared in terms of the Area-Time Square product versus other RNS and weighted binary structures. It is proven that existing combinatorial or look-up table approaches for RNS are tailored to small designs or special applications, while the pseudo-RNS approach remains competitive also for complex systems. {\textcopyright} 1995 IEEE},
author = {Claudio, Elio D.Di and Piazza, Francesco and Orlandi, Gianni},
doi = {10.1109/12.381948},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
keywords = {Binary multipliers,VLSI processors,digital signal processing,mixed radix arithmetic,parallel architectures,pseudo-residue,residue number system,systolic arrays},
number = {5},
pages = {624--633},
title = {{Fast Combinatorial RNS Processors for DSP Applications}},
volume = {44},
year = {1995}
}
@inproceedings{Dimitrov2001,
abstract = {A recently introduced double-base number representation tins proved to be successful in improving the performance of several algorithms in cryptography and digital signal processing. The index-calculus version of this number system can be regarded as a two-dimensional extension of the classical logarithmic number system. This paper builds on previous special results by generalizing the number system both in multiple dimensions (multiple bases) and by the use of multiple digits. Adopting both generalizations we show that large reductions in hardware complexity are achievable compared to an equivalent precision logarithmic number system.},
author = {Dimitrov, V. S. and Eskritt, J. and Imbert, L. and Jullien, G. A. and Miller, W. C.},
booktitle = {15th IEEE Symposium on Computer Arithmetic. ARITH-15 2001},
doi = {10.1109/arith.2001.930126},
issn = {1063-6889},
pages = {247--254},
title = {{The use of the multi-dimensional logarithmic number system in DSP applications}},
year = {2001}
}
@inproceedings{Chaves2003,
abstract = {This paper is focused on low power programmable fast digital signal processors (DSP) design based on a configurable 5-stage RISC core architecture and on residue number systems (RNS). Several innovative aspects are introduced at the control and datapath architecture levels, which support both the binary system and the RNS. A new moduli set {2n-1, 22n, 2n+1} is also proposed for balancing the processing time in the different RNS channels. Experimental results, obtained trough RDSP implementation on FPGA and ASIC, show that not only a significant reduction in circuit area and power consumption but also a speedup may be achieved with RNS when compared with a binary DSP.},
author = {Chaves, R. and Sousa, L.},
booktitle = {Proceedings - Euromicro Symposium on Digital System Design, DSD 2003},
doi = {10.1109/DSD.2003.1231911},
isbn = {0769520030},
pages = {128--135},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{RDSP: A RISC DSP based on residue number system}},
year = {2003}
}
@inproceedings{Cardarilli2007,
abstract = {In previous works ([1]-[8]) we performed different experiments implementing FIR filtering structures. Each filter was implemented using both the Two's Complement System (TCS) and the Residue Number System (RNS) number representations. The comparison of these two implementations allows to conclude that, for these applications, the RNS uses less power than the TCS counterpart. The aim of the present paper is to highlight the reasons of this power consumption reduction. {\textcopyright} 2007 IEEE.},
author = {Cardarilli, Gian Carlo and Nannarelli, Alberto and Re, Marco},
booktitle = {Conference Record - Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.2007.4487461},
isbn = {9781424421107},
issn = {10586393},
pages = {1412--1416},
publisher = {IEEE Computer Society},
title = {{Residue number system for low-power DSP applications}},
year = {2007}
}
@inproceedings{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia},
doi = {10.1145/2647868.2654889},
eprint = {1408.5093},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia et al. - 2014 - Caffe Convolutional architecture for fast feature embedding.pdf:pdf},
isbn = {9781450330633},
keywords = {Computer vision,Machine learning,Neural networks,Open source,Parallel computation},
month = {nov},
pages = {675--678},
publisher = {Association for Computing Machinery, Inc},
title = {{Caffe: Convolutional architecture for fast feature embedding}},
year = {2014}
}
@article{Schinianakis2014,
abstract = {A design methodology for incorporating Residue Number System (RNS) and Polynomial Residue Number System (PRNS) in Montgomery modular multiplication in GF(p) or GF(2n) respectively, as well as a VLSI architecture of a dual-field residue arithmetic Montgomery multiplier are presented in this paper. An analysis of input/output conversions to/from residue representation, along with the proposed residue Montgomery multiplication algorithm, reveals common multiply-accumulate data paths both between the converters and between the two residue representations. A versatile architecture is derived that supports all operations of Montgomery multiplication in GF(p) and GF(2n), input/output conversions, Mixed Radix Conversion (MRC) for integers and polynomials, dual-field modular exponentiation and inversion in the same hardware. Detailed comparisons with state-of-the-art implementations prove the potential of residue arithmetic exploitation in dual-field modular multiplication.},
author = {Schinianakis, Dimitrios and Stouraitis, Thanos},
doi = {10.1109/TCSI.2013.2283674},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schinianakis, Stouraitis - 2014 - Multifunction Residue Architectures for Cryptography.pdf:pdf},
journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
number = {4},
pages = {1156--1169},
title = {{Multifunction Residue Architectures for Cryptography}},
volume = {61},
year = {2014}
}
@article{Antao2014,
abstract = {Modular arithmetic is a building block for a variety of applications potentially supported on embedded systems. An approach to turn modular arithmetic more efficient is to identify algorithmic modifications that would enhance the parallelization of the target arithmetic in order to exploit the properties of parallel devices and platforms. The Residue Number System (RNS) introduces data-level parallelism, enabling the parallelization even for algorithms based on modular arithmetic with several data dependencies. However, the mapping of generic algorithms to full RNS-based implementations can be complex and the utilization of suitable hardware architectures that are scalable and adaptable to different demands is required. This paper proposes and discusses an architecture with scalability features for the parallel implementation of algorithms relying on modular arithmetic fully supported by the Residue Number System (RNS). The systematic mapping of a generic modular arithmetic algorithm to the architecture is presented. It can be applied as a high level synthesis step for an Application Specific Integrated Circuit (ASIC) or Field Programmable Gate Array (FPGA) design flow targeting modular arithmetic algorithms. An implementation with the Xilinx Virtex 4 and Altera Stratix II Field Programmable Gate Array (FPGA) technologies of the modular exponentiation and Elliptic Curve (EC) point multiplication, used in the Rivest-Shamir-Adleman (RSA) and (EC) cryptographic algorithms, suggests latency results in the same order of magnitude of the fastest hardware implementations of these operations known to date.},
author = {Ant{\~{a}}o, Samuel and Sousa, Leonel},
doi = {10.1007/s11265-014-0879-y},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ant{\~{a}}o, Sousa - 2014 - A Flexible Architecture for Modular Arithmetic Hardware Accelerators based on RNS.pdf:pdf},
issn = {19398115},
journal = {Journal of Signal Processing Systems},
keywords = {Cryptography,Electronic design automation (EDA),Embedded systems,Modular arithmetic,Residue number system (RNS)},
number = {3},
pages = {249--259},
title = {{A Flexible Architecture for Modular Arithmetic Hardware Accelerators based on RNS}},
volume = {76},
year = {2014}
}
@misc{Collet2015,
author = {Collet, Fanccois and Ohters},
title = {{Keras}},
url = {https://keras.io},
year = {2015}
}
@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
month = {mar},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@inproceedings{Szegedy2016,
abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - 2016 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781467388504},
issn = {10636919},
pages = {2818--2826},
publisher = {IEEE Computer Society},
title = {{Rethinking the Inception Architecture for Computer Vision}},
volume = {2016-Decem},
year = {2016}
}
@article{Miyashita2016,
abstract = {Recent advances in convolutional neural networks have considered model complexity and hardware efficiency to enable deployment onto embedded systems and mobile devices. For example, it is now well-known that the arithmetic operations of deep networks can be encoded down to 8-bit fixed-point without significant deterioration in performance. However, further reduction in precision down to as low as 3-bit fixed-point results in significant losses in performance. In this paper we propose a new data representation that enables state-of-the-art networks to be encoded to 3 bits with negligible loss in classification performance. To perform this, we take advantage of the fact that the weights and activations in a trained network naturally have non-uniform distributions. Using non-uniform, base-2 logarithmic representation to encode weights, communicate activations, and perform dot-products enables networks to 1) achieve higher classification accuracies than fixed-point at the same resolution and 2) eliminate bulky digital multipliers. Finally, we propose an end-to-end training procedure that uses log representation at 5-bits, which achieves higher final test accuracy than linear at 5-bits.},
archivePrefix = {arXiv},
arxivId = {1603.01025},
author = {Miyashita, Daisuke and Lee, Edward H. and Murmann, Boris},
eprint = {1603.01025},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miyashita, Lee, Murmann - 2016 - Convolutional Neural Networks using Logarithmic Data Representation.pdf:pdf},
month = {mar},
title = {{Convolutional Neural Networks using Logarithmic Data Representation}},
url = {http://arxiv.org/abs/1603.01025},
year = {2016}
}
@techreport{Paszke2017,
abstract = {In this article, we describe an automatic differentiation module of PyTorch-a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and Facebook, Zachary Devito and Research, A I and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Srl, Orobix and Lerer, Adam},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paszke et al. - 2017 - Automatic differentiation in PyTorch.pdf:pdf},
month = {oct},
title = {{Automatic differentiation in PyTorch}},
year = {2017}
}
@article{Bezanson2017,
abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse helds of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow. 2. One must prototype in one language and then rewrite in another language for speed or deployment. 3. There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. We introduce the Julia programming language and its design-a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
archivePrefix = {arXiv},
arxivId = {1411.1607},
author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
doi = {10.1137/141000671},
eprint = {1411.1607},
issn = {0036-1445},
journal = {SIAM Review},
keywords = {Julia,Numerical,Parallel,Scientific computing},
month = {feb},
number = {1},
pages = {65--98},
publisher = {Society for Industrial and Applied Mathematics Publications},
title = {{Julia: A fresh approach to numerical computing}},
volume = {59},
year = {2017}
}
@article{Gustafson2017,
abstract = {A new data type called a posit is designed as a direct drop-in replacement for IEEE Standard 754 floating-point numbers (floats). Unlike earlier forms of universal number (unum) arithmetic, posits do not require interval arithmetic or variable size operands; like floats, they round if an answer is inexact. However, they provide compelling advantages over floats, including larger dynamic range, higher accuracy, better closure, bitwise identical results across systems, simpler hardware, and simpler exception handling. Posits never overflow to infinity or underflow to zero, and "Nota- Number" (NaN) indicates an action instead of a bit pattern. A posit processing unit takes less circuitry than an IEEE float FPU. With lower power use and smaller silicon footprint, the posit operations per second (POPS) supported by a chip can be significantly higher than the FLOPS using similar hardware resources. GPU accelerators and Deep Learning processors, in particular, can do more per watt and per dollar with posits, yet deliver superior answer quality. A comprehensive series of benchmarks compares floats and posits for decimals of accuracy produced for a set precision. Low precision posits provide a better solution than "approximate computing" methods that try to tolerate decreased answer quality. High precision posits provide more correct decimals than floats of the same size; in some cases, a 32-bit posit may safely replace a 64-bit float. In other words, posits beat floats at their own game.},
author = {Gustafson, John L. and Yonemoto, Isaac},
doi = {10.14529/jsfi170206},
file = {:home/mhizzani/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gustafson, Yonemoto - 2017 - Beating floating point at its own game Posit arithmetic.pdf:pdf},
issn = {2313-8734},
journal = {Supercomputing Frontiers and Innovations},
keywords = {Computer arithmetic,Energy-efficient computing,Floating point,LINPACK,Linear algebra,Neural networks,Posits,Unum computing,Valid arithmetic},
number = {2},
pages = {71--86},
publisher = {South Ural State University, Publishing Center},
title = {{Beating floating point at its own game: Posit arithmetic}},
volume = {4},
year = {2017}
}
@article{Flux.jl-2018,
archivePrefix = {arXiv},
arxivId = {1811.01457},
author = {Innes, Michael and Saba, Elliot and Fischer, Keno and Gandhi, Dhairya and Rudilosso, Marco Concetto and Joy, Neethu Mariya and Karmali, Tejan and Pal, Avik and Shah, Viral},
eprint = {1811.01457},
journal = {CoRR},
title = {{Fashionable Modelling with Flux}},
url = {http://arxiv.org/abs/1811.01457},
volume = {abs/1811.0},
year = {2018}
}
@article{Innes2018,
author = {Innes, Mike},
doi = {10.21105/joss.00602},
journal = {Journal of Open Source Software},
title = {{Flux: Elegant Machine Learning with Julia}},
year = {2018}
}
@article{Asif2018a,
abstract = {This work proposes a residue number system (RNS) based hardware architecture of an elliptic curve cryptography (ECC) processor over prime field. The processor computes point multiplication in Jacobian coordinates by a combined architecture of point doubling and point addition. An optimized modular reduction architecture is also presented that achieves low area by dividing the RNS moduli in small groups and processes the groups one by one. The proposed ECC processor is generic and supports any random curve over Fp256. The implementation on Virtex-7 and Virtex-6 FPGAs shows comparable performance to the state-of-the-art binary and RNS based ECC processors.},
author = {Asif, Shahzad and Hossain, Md Selim and Kong, Yinan and Abdul, Wadood},
doi = {10.1016/J.VLSI.2017.11.010},
issn = {0167-9260},
journal = {Integration},
pages = {138--149},
publisher = {Elsevier},
title = {{A Fully RNS based ECC Processor}},
volume = {61},
year = {2018}
}
@incollection{PyTorch2019,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d\textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {8024--8035},
publisher = {Curran Associates, Inc.},
title = {{PyTorch: An Imperative Style, High-Performance Deep Learning Library}},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
year = {2019}
}
@inproceedings{Juang2019,
abstract = {In this paper, we have proposed area-delay product (ADP) efficient design for convolutional neural network (CNN) circuits using logarithmic number systems (LNS). By employing LNS-based schemes, the area overhead for large amount of conventional multipliers required in CNN circuits can be tremendously reduced. Simulation results show that our proposed design can achieve lower-error with almost 60% ADP savings compared with the conventional multipliers-based design, which is suitable for deep learning applications.},
address = {Daegu},
author = {Juang, Tso Bing and Lin, Cong Yi and Lin, Guan Zhong},
booktitle = {2018 International SoC Design Conference (ISOCC)},
doi = {10.1109/ISOCC.2018.8649961},
isbn = {978-1-5386-7960-9},
keywords = {Computer arithmetic,Convolutional neural network (CNN),Deep learning,Logarithmic number system (LNS),VLSI design},
pages = {170--171},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Area-Delay Product Efficient Design for Convolutional Neural Network Circuits Using Logarithmic Number Systems}},
year = {2019}
}
@phdthesis{Hizzani2019,
author = {Hizzani, Mohammad},
booktitle = {Princess Sumaya University for Technology},
isbn = {978-1697960471},
school = {Princess Sumaya University for Technology},
title = {{Design of a Modular Multiplier for Public-Key Cryptography Applications Using Residue Number System and Signed-Digit Representation}},
type = {Master Thesis},
year = {2019}
}
@article{754,
isbn = {9781504459242},
journal = {IEEE Std 754-2019 (Revision of IEEE 754-2008)},
pages = {1--84},
publisher = {IEEE},
title = {{754-2019 - IEEE Standard for Floating-Point Arithmetic}},
year = {2019}
}
@article{Samimi2020,
abstract = {In this article, a technique, based on using Residue Number System (RNS) is suggested to improve the energy efficiency of Deep Neural Networks (DNNs). In the DNN architecture, which is fully RNS-based, only weights and the primary inputs in the main memory are in the binary number system (BNS). The architecture, which is called Res-DNN, offers a high energy saving while requiring higher bit count for data to handle the overflow compared to that of a BNS one. Scaling techniques in the processing elements are employed in the RNS-based computations to make the computation bit widths the same as the BNS bit width. In this architecture, the MAX pooling and ReLU activation function are implemented in the RNS format. To lower the memory usage and required memory bandwidth, we suggest a Huffman-based coding. Additionally, for accessing the weights stored in the main memory, to obtain further energy reduction, we propose a structural modification to the memory hierarchy where a lower level register file is added to the data path of these accesses. The effectiveness of the proposed architecture is evaluated under seven state-of-the-art DNNs with the datasets of ImageNet and CIFAR-10. The obtained results show that Res-DNN leads to 2.5 × lower energy for computations and an average of 30% overall energy reduction compared to those of the binary counterpart.},
author = {Samimi, Nasim and Kamal, Mehdi and Afzali-Kusha, Ali and Pedram, Massoud},
doi = {10.1109/TCSI.2019.2951083},
issn = {1558-0806},
journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
keywords = {Residue number system,architecture,deep neural network,energy},
number = {2},
pages = {658--671},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Res-DNN: A residue number system-based DNN accelerator unit}},
volume = {67},
year = {2020}
}
