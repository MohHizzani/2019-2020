\section{Introduction}

%\IEEEPARstart{M}{any}
Many new and old number systems have been introduced as an alternative to IEEE-754 standards \cite{754} for deeplearning. Some of them have been used in many different fields of Digital Signal Processing (DSP) and other specific applications, namely for and to process several types of neural networks (i.e. Fully-connected layers, CNN and RNN).

\subsection{Examples of Number Systems For Deeplearning}

An example of a non-conventional number systems is the Posit number system, which is known also as UNUM III \cite{Gustafson2017}. Posit was introduced as an advanced number system that overpasses IEEE standards as a hardware-friendly system for general-purpose arithmetic. It also provides faster versions of functions used in neural networks such as the sigmoid function\footnote{Sigmoid function $\left(\sigma(x) = \frac{1}{1 + e^{-1}}\right)$ is an activation function, the $\tanh$ function is widely used and it is a scaled and shifted version of the Sigmoid function $\left(\tanh(x) = 2 \sigma(2x) -1\right)$.}.

Some suggested number systems can be modified to perform faster computation than the conventional IEEE-754 such as Logarithmic Number Systems (LNS) \cite{Kingsbury1971} and Residual Number System (RNS) \cite{Garner1959}. The RNS has been being used in many DSP applications \cite{Cardarilli2007,Chaves2003,Claudio1995,DiClaudio1990,Jullien1987} and other extensive computational applications like asymmetric (public-key) cryptography \cite{Sousa2016}. RNS was used to provide energy-saving units in Convolution Neural Networks (CNN) \cite{Samimi2020}.

LNS \cite{Kingsbury1971,Alexopoulos1975,Lee1977} has also been used in many applications including DSP \cite{Dimitrov2001,Lewis1995}. Recently it has been developed to be used in CNN, in order to provide high accuracy in training using significantly smaller bit length \cite{Miyashita2016,Juang2019}.

\subsection{Why The Julia Programming Language}

Julia \cite{Julia,Bezanson2017} is a high-level programming language that was designed for numerical and scientific computation, well suited for data science. It provides an unconventional way of defining primitive types at hardware level (with number of bits predefined), where all data types of Julia defined, designed and implemented using only native Julia\footnote{\mintinline{julia}{Float16, Float32, Float64} represent the IEEE Floating types in Julia. \mintinline{julia}{Int128, Int16, Int32, Int64, Int8} are the conventional Signed-Integers similarly \mintinline{julia}{UInt128, UInt16, UInt32, UInt64, UInt8} are the conventional Unsigned integers.}.

Moreover, Julia provides a novel way of handling \emph{multiple dispatch} \cite{WikiMultipleDispatch} concept in programming languages \cite{JuliaMehtods}. Multiple Dispatch in a nutshell is that a function with a single name has different processes (methods) base on its argument(s). Examples of multiple dispatch are the primitive operations (addition, subtraction, multiplication and division) as follows:

\begin{listing}[H]
\begin{minted}{julia}
julia> aInt, bInt = 1, 2; #these are Integers

julia> aF, bF = 1.0, 2.0; #there are Floats

julia> aInt + bF #this will call the method +(::Integer, ::Float64) note the result is in Float64
3.0
\end{minted}
\caption{Multiple Dispatch Example}
\end{listing}

Note how the result was promoted to be in \mintinline{julia}{Float64} because of Julia special promotion functions (\mintinline[fontsize=\footnotesize ]{julia}{promote_type(::Type{Integer},::Type{Float64}) = Float64}). This means that functions used in any package can be used without making any modification to the package, it is only required to define methods of those functions for the new type. For instance, functions such as trigonometric functions or hyperbolic functions as by default provided by Julia for its primitive types including the IEEE-754. If any of these exists in a package simply define any of them for the new number system that is to be used in this package, further details will be given in subsection~\ref{subsec:saa}.

\textbf{Divide and conquer} is an essential approach of development, which facilitates the focus on smaller pieces of a development process by dividing a system into layers, each layer is easily analyzed and improved. Hence, this property of Julia (Multiple Dispatch) makes it easier to separate different steps, and then integration with it is a separated process. This means that less time is used on editing and modifying, and more time is used for designing, testing and developing. The upcoming sections will present the methods of functions of NumNN.jl with which any new number system should be defined beforehand.

Furthermore, Julia provides a native support for parallel operations, namely co-routines, multi-threads and multi-core. Mostly, no new code is needed for parallel operations, the same code with some macros can be used to process in parallel. In the same way, for-loops in Julia are as fast as for-loops in C programming language. 
