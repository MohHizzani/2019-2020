\section{Introduction}

%\IEEEPARstart{M}{any}
Many new and old number systems have been introduced as a better alternative to IEEE-754 standards \cite{754} in the field of Deeplearning. Some were used in many different fields of Digital Signal Processing (DSP) or other specific application systems and for the sake of neural networks they have been modified to process all or some types of neural networks (i.e. Fully-connected layers, CNN and RNN).

\subsection{Examples of Number Systems Used in Deeplearning}

An example of novel number systems is the Posit number system \cite{Gustafson2017}, which is known also as UNUM III. Posit was introduced as an advanced number system that overpasses IEEE standards as a hardware-friendly system for general-purpose computational arithmetic operations. It also provides some faster versions of functions used in neural networks such as the Sigmoid function\footnote{Sigmoid function $\left(\sigma(x) = \frac{1}{1 + e^{-1}}\right)$ is rarely used as an activation function nowadays. However, $\tanh$ function is widely used and it is a scaled and shifted version of the Sigmoid function $\left(\tanh(x) = 2 \sigma(2x) -1\right)$.}.

Some suggest to use some previously known number systems that can be modified to perform much faster computation than the conventional IEEE-754. Residual Number System (RNS) \cite{Garner1959} has been being used in many DSP applications \cite{Cardarilli2007,Chaves2003,Claudio1995,DiClaudio1990,Jullien1987} and other extensive computational applications like asymmetric cryptography \cite{Hizzani2019,Asif2018a,Schinianakis2014,Antao2014}. RNS was used as a potential number system to provide energy-saving units in Convolution Neural Networks (CNN) \cite{Samimi2020}.

Another well-known number system is the Logarithmic Number System (LNS) \cite{Kingsbury1971,Alexopoulos1975,Lee1977} which also has been being used in many applications including DSP \cite{Dimitrov2001,Lewis1995}. Recently it has been developed to be used in CNN to provide higher accuracy in training using significantly smaller bit length \cite{Miyashita2016,Juang2019}.

\subsection{Why Julia?}

Julia \cite{Julia,Bezanson2017} is a high-level programming language that was designed with scientific research and data science in mind. It provides an unconventional way of defining primitive types at hardware level (with number of bits predefined) where all data types of Julia were defined, designed and implemented using only native Julia\footnote{\mintinline{julia}{Float16, Float32, Float64} represent the IEEE Floating types in Julia. \mintinline{julia}{Int128, Int16, Int32, Int64, Int8} are the conventional Signed-Integers similarly \mintinline{julia}{UInt128, UInt16, UInt32, UInt64, UInt8} are the conventional Unsigned integers.}.

Moreover, Julia provides a novel way of handling \emph{multiple dispatch} \cite{WikiMultipleDispatch} concept in programming languages \cite{JuliaMehtods}. Multiple Dispatch in a nutshell is that a function with a single name has different processes (methods) base on its argument(s). Examples of multiple dispatch are the primitive operations (addition, subtraction, multiplication and division) as follows:

\begin{listing}[H]
\begin{minted}{julia}
julia> aInt, bInt = 1, 2; #these are Integers

julia> aF, bF = 1.0, 2.0; #there are Floats

julia> aInt + bF #this will call the method +(::Integer, ::Float64) note the result is in Float64
3.0
\end{minted}
\caption{Multiple Dispatch First Example}
\end{listing}

Note how the result was promoted to be in \mintinline{julia}{Float64} because of Julia special promotion functions (\mintinline[fontsize=\footnotesize ]{julia}{promote_type(::Type{Integer},::Type{Float64}) = Float64}). This means that functions used in any package can be used without making any modification to the package, it is only required to define methods of those functions for the new type. For instance, functions such as Trigonometric functions or Hyperbolic functions as by default provided by Julia for its primitive types including the IEEE-754 types. If any of these are used in a package simply define any of them for the new number system that is to be used in this package, for further details see subsection~\ref{subsec:saa}.

\textbf{Divide and conquer} is an essential approach of development, which facilitates the focus on smaller pieces of a development process by dividing a system into layers, this way each layer is being studied and improved. Hence, this property of Julia (Multiple Dispatch) makes it much easier to separate different steps of development by developing a package and then integration with it will be separated of the source code. This means that less time is used on editing and modifying, and more time is used for designing, testing and developing. In the upcoming sections it will be presented what methods of functions of NumNN.jl with which any new number system should be defined beforehand.

Furthermore, Julia provides a native support for parallel operations namely co-routines, multi-threads and multi-core. Mostly no new code is needed for parallel operations, and when needed, the same code with some macros do the job. In the same way, for-loops in Julia are extremely fast as fast as for-loops of C-like languages. 
