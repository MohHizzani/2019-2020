\section{NumNN.jl}

This package\footnote{\href{https://github.com/MohHizzani/NumNN.jl}{https://github.com/MohHizzani/NumNN.jl}} (library) was built on the concept of Rapid Prototyping to enable fast experimentation. \emph{Begin able to go from idea to result with the least possible delay is key to good research}\cite{Keras}.

\subsection{Fast Implementation and Deployment}

NumNN.jl provides the fastest possible way to implement a neural network model, train and use it for prediction and testing. Installing the package and using it can be done from a Julia REPL as follows:

\begin{listing}[H]
	\begin{minted}{julia}
	julia> ] add NumNN
	julia> using NumNN
	\end{minted}
	\caption{Adding NumNN.jl and import it}\label{addimport}
\end{listing}

Defining layers sequence can be easily done either like this:

\begin{listing}[H]
	\begin{minted}{julia}
	X_Input, X_Ouput = chain(X_train,[Flatten(),FCLayer(120,:relu),FCLayer(10,:softmax)])
	\end{minted}
	\caption{Chained Layers with no side branch(es)}\label{chain}
\end{listing}


where \mintinline[fontsize=\footnotesize]{julia}{X_train} is the train array or the size of train array. Or in case there are some side branches it can be easily done as the following example of Inception Net \cite{Szegedy2016}:

\begin{listing}[H]
	\begin{minted}{julia}
	X_Input = Input(X_train)
	Xc = [
	Conv2D(3, (3,3), padding=:same)(X_Input),
	Conv2D(4, (5,5), padding=:same)(X_Input),
	Conv2D(10, (1,1), padding=:same)(X_Input),
	MaxPool2D((2,2); padding=:same)(X_Input),
	AveragePool2D((3,3); padding=:same)(X_Input),
	]
	X = ConcatLayer()(Xc)
	X = BatchNorm(dim=3)(X) #to normalize across the channels
	X = Activation(:relu)(X)
	X = MaxPool2D((2,2))(X);
	Xc = [
	Conv2D(6, (3,3), padding=:same)(X),
	Conv2D(8, (5,5), padding=:same)(X),
	Conv2D(10, (1,1), padding=:same)(X),
	MaxPool2D((2,2); padding=:same)(X),
	AveragePool2D((3,3); padding=:same)(X),
	]
	X = ConcatLayer()(Xc)
	X = BatchNorm(dim=3)(X) #to normalize across the channels
	X = Activation(:relu)(X)
	X = AveragePool2D((3,3))(X);
	X = Flatten()(X)
	X_Output = FCLayer(10, :softmax)(X);
	\end{minted}
	\caption{InceptionNet Example}\label{chain}
\end{listing}

%A group of examples can be found in Appendices.

\subsection{Structure and Architecture}

The structure of NumNN.jl is simple enough to comprehend in few minutes and to start amend in case any new element is needed to be added. \figurename\ref{fig:layerstruct} shows the Layers hierarchy in NumNN.jl, this structure was built to facilitate the use and the future development of NumNN.jl.

\begin{figure}[!ht]
	\centering
	%\begin{subfigure}{0.95\textwidth}
		\input{typetree}
	%\end{subfigure}
	\caption{Layer Architecture in NumNN.jl, the useable layers are the leaves the other nodes are \mintinline[fontsize=\footnotesize]{julia}{abstract type}s.}\label{fig:layerstruct}
\end{figure}

An essential data type (\mintinline[fontsize=\footnotesize]{julia}{Model}) holds the main pointers to layers structure and parameters to use. Instantiating a (\mintinline[fontsize=\footnotesize]{julia}{Model}) will also invoke the initialization of the layers scaling and bias parameters and can be done as follows:

\begin{listing}[H]
	\begin{minted}{julia}
	model = Model(X_train, Y_train, X_Input, X_Output, 0.001; optimizer=:adam, loss=:categoricalCrossentropy)
	\end{minted}
	\caption{Model initialization, \mintinline{julia}{X_train, Y_train} are training data and labels, while \mintinline{Julia}{X_Input, X_Ouput} are the input and output layers. The value of \mintinline{julia}{0.001} represent the learning rate of this model, where the key-word \mintinline{julia}{optimizer} define the optimizer to use during training, and \mintinline{julia}{loss} defines the loss function.}\label{modelinit}
\end{listing}

Activation functions are defined under the \mintinline[fontsize=\footnotesize]{julia}{abstract type actFun}. NumNN.jl only provided few activation functions (sigmoid ($\sigma$), \mintinline{julia}{tanh}, \mintinline[fontsize=\footnotesize]{julia}{softmax} and \mintinline[fontsize=\footnotesize]{julia}{noAct}). This is because using any other activation function can be defined as a subtype to \mintinline[fontsize=\footnotesize]{julia}{actFun} and then as a normal function with another function for its derivative. For instance, if you want to add $\sin$ as an activation function all what you need to do is:

\begin{listing}[H]
	\begin{minted}{julia}
	abstract type sin <: actFun end
	
	sin(x) = base.sin.(x) #note the use of base. to tell julia that we need the default sin at this step not the type defined because it was overwritten
	dsin(x) = base.cos.(x)
	\end{minted}
	\caption{Example of defining a new activation function to NumNN.jl}
\end{listing}

NumNN.jl has loss functions under the \mintinline{julia}{abstract type lossFun}. The two main loss functions defined are the \mintinline{julia}{binaryCrossentropy} and \mintinline{julia}{categoricalCrossentropy}. Also, we believe other loss functions can be easily defined, and more functions are coming in the upcoming versions of NumNN.jl.

%\subsection{Sequence of Processes}

