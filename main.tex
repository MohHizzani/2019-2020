%\documentclass[onecolumn]{IEEEtran}
\documentclass{article}
\usepackage[nonatbib]{neurips_2020}
%\usepackage{natbib}
\usepackage[utf8]{inputenc}

\usepackage{IEEEtrantools}
%\usepackage{standalone}
%\include{examplepreamble}
%\standaloneconfig{}
%\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{framed}

%\usepackage[hidelinks]{hyperref}

%\usepackage{cite}

\usepackage{caption}
%\usepackage{graphicx}
%\usepackage{float}
%\usepackage{listing}
\usepackage[dvipsnames]{xcolor}
\definecolor{LGray}{gray}{0.98}
\definecolor{dgray}{gray}{0.3}
\usepackage{scrextend}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	citecolor=dgray,
}



%%% for code wraping
\usepackage{minted}
\usemintedstyle{colorful}
\setminted[julia]{frame=lines,rulecolor=MidnightBlue,bgcolor=LGray,fontsize=\footnotesize ,breaklines}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{mindmap}

\tikzstyle{layer} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]


\usepackage{pmboxdraw}
%\usepackage{qtree}
%\usepackage{subcaption}


\usepackage{array}
\usepackage{multirow}

\title{NumNN.jl: A Toolkit for Simulating Deeplearning Networks Supporting Non-Conventional Arithmetic}

\author{%\IEEEauthorblockN
	{Mohammad~Hizzani}\\
	%\IEEEauthorblockA
	%{
	INESC-ID\\ 
	Instituto Superior Técnico\\ 
		Universidade de Lisboa\\
		Email: \href{mailto:moh.hizzani@gmail.com}{\tt moh.hizzani@gmail.com}%}
	\And
	%\IEEEauthorblockN
	{Leonel Sousa}\\
	%\IEEEauthorblockA
	%{
	INESC-ID\\ 
		Instituto Superior Técnico\\ 
		Universidade de Lisboa\\
		Email: \href{mailto:las@inesc-id.pt}{\tt las@inesc-id.pt}%}
	%Email: some@sdf.com}
}


\begin{document}
	\maketitle

	\begin{abstract}
		The development of Deeplearning models requires hardware support, with deep architectures (organized around different number of layers and computations per layer whether they are nodes of fully-connected layers or channels of convolutional neural networks). Moreover, deeplearning models use big data for training, with the need for tuning the hyper parameters, requiring the repetition of the whole training process multiple times. Thus, hardware-friendly number systems and arithmetic have been introduced alternatively to the IEEE floating-point formats, which are cost effective and/or more accurate. However, the hardware design, verification and assessment are more complex and expensive processes. Hence, these arithmetic systems should be tested in simulation environments for the target applications, before moving forward to system design. Since most deeplearning frameworks and libraries target AI production, they only support the IEEE formats. This paper presents NumNN.jl, a tool for supporting and facilitating the design and prototyping of deeplearning systems based on different number representations. It provides an agile way of designing, training and evaluating any type of deeplearning (neural network) model with any number and arithmetic system(s).
	\end{abstract}

	\input{introduction}
	
	\input{body}
	
	\input{futurework}
%	\includestandalone[subpreambles=true]{00-FCLayer-FashionMNIST/00-FCLayer-FashionMNIST}
	%\input{appendices}
	
	\bibliographystyle{IEEEtran}
	%\bibliographystyle{plain}
	\bibliography{main}
	%\printbibliography
\end{document}
