%\documentclass[onecolumn]{IEEEtran}
\documentclass{article}
\usepackage[nonatbib]{neurips_2020}
%\usepackage{natbib}
\usepackage[utf8]{inputenc}

\usepackage{IEEEtrantools}
%\usepackage{standalone}
%\include{examplepreamble}
%\standaloneconfig{}
%\usepackage[subpreambles=true]{standalone}
\usepackage{import}
\usepackage{framed}

%\usepackage[hidelinks]{hyperref}

%\usepackage{cite}

\usepackage{caption}
%\usepackage{graphicx}
%\usepackage{float}
%\usepackage{listing}
\usepackage[dvipsnames]{xcolor}
\definecolor{LGray}{gray}{0.98}
\definecolor{dgray}{gray}{0.3}
\usepackage{scrextend}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,      
	urlcolor=cyan,
	citecolor=dgray,
}



%%% for code wraping
\usepackage{minted}
\usemintedstyle{colorful}
\setminted[julia]{frame=lines,rulecolor=MidnightBlue,bgcolor=LGray,fontsize=\footnotesize ,breaklines}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\usetikzlibrary{mindmap}

\tikzstyle{layer} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]


\usepackage{pmboxdraw}
%\usepackage{qtree}
%\usepackage{subcaption}


\usepackage{array}
\usepackage{multirow}

\title{NumNN.jl: A Toolkit for Simulating Deeplearning Networks Supporting Non-Conventional Arithmetic}

\author{%\IEEEauthorblockN
	{Mohammad~Hizzani}\\
	%\IEEEauthorblockA
	%{
	INESC-ID\\ 
	Instituto Superior Técnico\\ 
		Universidade de Lisboa\\
		Email: \href{mailto:moh.hizzani@gmail.com}{\tt moh.hizzani@gmail.com}%}
	\And
	%\IEEEauthorblockN
	{Leonel Sousa}\\
	%\IEEEauthorblockA
	%{
	INESC-ID\\ 
		Instituto Superior Técnico\\ 
		Universidade de Lisboa\\
		Email: \href{mailto:las@inesc-id.pt}{\tt las@inesc-id.pt}%}
	%Email: some@sdf.com}
}


\begin{document}
	\maketitle

	\begin{abstract}
		The development of Deeplearning models requires hardware support, with deep architectures organized around different number of layers, whether they are fully-connected networks or convolutional. Moreover, deeplearning models use big data for training, with the need for tuning the hyper parameters, requiring the repetition of the whole training process multiple times. Thus, hardware-friendly number systems and arithmetic have been introduced alternatively to the IEEE floating-point formats, which are cost effective and/or more accurate. However, the hardware design, verification and assessment are complex and expensive processes. Hence, these arithmetic systems should be tested in simulation environments for the target applications, before moving forward to system design. Most deeplearning frameworks and libraries target AI production, they are supporting the IEEE floating point formats. This paper presents NumNN.jl, a tool for facilitating the design and prototyping of deeplearning systems based on different number representations. It provides an agile way of designing, training and evaluating any type of deeplearning (neural network) model with any number system. NumNN.jl has been used to develop deeplearning networks supported not only on 32-bit and 64-bit IEEE floating point formats but also on the Posit number representation.  Results, obtained with NumNN.jl for benchmarks, are presented and compared with TensorFlow, showing the practical interest of the proposed software tool.
	\end{abstract}

	\input{introduction}
	
	\input{body}
	
	\input{futurework}
%	\includestandalone[subpreambles=true]{00-FCLayer-FashionMNIST/00-FCLayer-FashionMNIST}
	%\input{appendices}
	
	\bibliographystyle{IEEEtran}
	%\bibliographystyle{plain}
	\bibliography{main}
	%\printbibliography
\end{document}
